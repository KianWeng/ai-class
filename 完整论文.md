# 基于YOLO的智慧课堂行为检测系统

## 第一部分：模型训练

### 1. 引言

随着人工智能技术的快速发展，计算机视觉在教育领域的应用日益广泛。智慧课堂系统通过实时监测和分析学生行为，为教师提供课堂管理辅助工具，有助于提高教学质量和学习效果。本系统基于YOLO（You Only Look Once）目标检测算法，实现了人脸检测和学生行为检测功能，为智慧课堂提供了可靠的技术支撑。

### 2. 相关工作

#### 2.1 YOLO算法简介

YOLO是一种端到端的目标检测算法，其核心思想是将目标检测问题转化为回归问题，通过单次前向传播即可完成目标定位和分类。相比传统的两阶段检测算法（如R-CNN系列），YOLO具有检测速度快、实时性好的优势，非常适合视频流实时检测场景。

本项目采用Ultralytics公司开发的YOLO11框架，该框架在YOLOv8的基础上进一步优化，提供了更好的检测精度和推理速度。

#### 2.2 应用场景

- **人脸检测**：在课堂环境中实时检测学生人脸，为后续的人脸识别和考勤管理提供基础
- **行为检测**：检测学生的课堂行为，如低头、转头等，帮助教师及时发现学生的注意力状态

### 3. 数据集准备

#### 3.1 人脸检测数据集

本项目使用WIDER FACE数据集进行人脸检测模型的训练。WIDER FACE是一个大规模的人脸检测基准数据集，包含32,203张图像和393,703个人脸标注，涵盖了各种尺度、姿态和遮挡情况。

**数据集结构：**
```
data/face_detect/
├── WIDER_train.zip          # 训练集（1.4GB）
├── WIDER_val.zip            # 验证集（346MB）
├── wider_face_split/        # 标注文件
│   ├── wider_face_train.mat
│   ├── wider_face_val.mat
│   └── ...
└── widerface.yaml           # 数据集配置文件
```

**数据集配置（widerface.yaml）：**
```yaml
path: /home/zephyr/ai-class/model_training/data/face_detect/widerface
train: train
val: val
test:

names:
  0: face
```

#### 3.2 行为检测数据集

本项目针对两种学生行为进行检测：
1. **BowHead（低头）**：学生低头看手机或书本的行为
2. **TurnHead（转头）**：学生转头与他人交流的行为

**SCB_BTH数据集（BowHead & TurnHead）：**
- 训练集和验证集图像位于 `data/scb_bth/images/` 目录
- 标注文件位于 `data/scb_bth/labels/` 目录
- 数据集大小：301MB

**SCB_HRW数据集（Handrise-Read-write）：**
- 训练集和验证集图像位于 `data/scb_hrw/images/` 目录
- 标注文件位于 `data/scb_hrw/labels/` 目录
- 数据集大小：744MB

**数据集配置示例（scb_bth.yaml）：**
```yaml
names:
- 'BowHead'
- 'TurnHead'
nc: 2
train: /home/zephyr/ai-class/model_training/data/scb_bth/images/train
val: /home/zephyr/ai-class/model_training/data/scb_bth/images/val
```

### 4. 模型训练

#### 4.1 训练脚本

本项目使用Ultralytics YOLO框架进行模型训练，训练脚本位于 `model_training/train.py`。

**核心代码：**
```python
import argparse
from ultralytics import YOLO

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', default='yolov8n.pt', help='Initial weights path')
    parser.add_argument('--data', default='data/widerface.yaml', help='Path to data file')
    parser.add_argument('--epochs', type=int, default=300)
    parser.add_argument('--batch', type=int, default=16)
    parser.add_argument('--imgsz', type=int, default=640, help='Image size')
    parser.add_argument('--device', default='0', help='CUDA device')
    parser.add_argument('--resume', type=bool, default=False)
    parser.add_argument('--optimizer', default=None)
    parser.add_argument('--lrf', type=float, default=None)
    parser.add_argument('--weight-decay', type=float, default=None)
    opt = parser.parse_args()

    model = YOLO(opt.weights)
    model.train(
        data=opt.data,
        epochs=opt.epochs,
        batch=opt.batch,
        imgsz=opt.imgsz,
        device=opt.device,
        resume=opt.resume
    )
```

#### 4.2 训练参数配置

**人脸检测模型训练：**
```bash
python train.py \
    --weights yolov8n.pt \
    --data data/face_detect/widerface.yaml \
    --epochs 300 \
    --batch 16 \
    --imgsz 640 \
    --device 0
```

**行为检测模型训练（SCB_BTH）：**
```bash
python train.py \
    --weights yolov8n.pt \
    --data data/scb_bth/scb_bth.yaml \
    --epochs 300 \
    --batch 16 \
    --imgsz 640 \
    --device 0
```

**行为检测模型训练（SCB_HRW）：**
```bash
python train.py \
    --weights yolov8n.pt \
    --data data/scb_hrw/scb_hrw.yaml \
    --epochs 300 \
    --batch 16 \
    --imgsz 640 \
    --device 0
```

**主要训练参数说明：**
- `--weights`: 预训练模型权重，使用YOLOv8n作为基础模型
- `--data`: 数据集配置文件路径
- `--epochs`: 训练轮数，默认300轮
- `--batch`: 批次大小，根据GPU显存调整
- `--imgsz`: 输入图像尺寸，默认640×640
- `--device`: 训练设备，'0'表示使用第一块GPU，'cpu'表示使用CPU

#### 4.3 训练过程

训练过程中，YOLO框架会自动进行以下操作：

1. **数据加载**：根据配置文件加载训练集和验证集
2. **数据增强**：自动应用随机翻转、缩放、色彩调整等数据增强技术
3. **模型训练**：使用反向传播算法优化模型参数
4. **验证评估**：每个epoch结束后在验证集上评估模型性能
5. **模型保存**：自动保存最佳模型权重（best.pt）和最新权重（last.pt）

**训练输出：**
训练过程会在 `runs/detect/` 目录下生成以下文件：
- `weights/best.pt`: 验证集上表现最好的模型权重
- `weights/last.pt`: 最后一个epoch的模型权重
- `results.png`: 训练曲线图（损失函数、mAP等指标）
- `confusion_matrix.png`: 混淆矩阵
- `args.yaml`: 训练参数配置文件

#### 4.4 模型评估指标

YOLO框架在训练过程中会计算以下评估指标：

- **mAP (mean Average Precision)**：平均精度均值，衡量检测精度
- **Precision**：精确率，检测为正样本中真正为正样本的比例
- **Recall**：召回率，所有正样本中被正确检测的比例
- **Box Loss**：边界框回归损失
- **Cls Loss**：分类损失
- **Dfl Loss**：分布焦点损失

### 5. 训练结果

#### 5.1 人脸检测模型

训练完成后，人脸检测模型能够准确检测图像和视频中的人脸，为后续的人脸识别功能提供基础。模型保存在 `demo/weights/face_detect/best.pt`。

**模型特点：**
- 支持多尺度人脸检测
- 对光照、角度、遮挡等变化具有较好的鲁棒性
- 检测速度快，适合实时应用

#### 5.2 行为检测模型

**SCB_BTH模型（低头和转头检测）：**
- 能够准确识别学生的低头（BowHead）和转头（TurnHead）行为
- 模型保存在 `demo/weights/scb/bth_best.pt`

**SCB_HRW模型（举手、阅读、写字检测）：**
- 能够识别学生的举手、阅读、写字等行为
- 模型保存在 `demo/weights/scb/hrw_best.pt`

### 6. 模型优化与改进

#### 6.1 数据增强策略

训练过程中采用了多种数据增强技术：
- 随机水平翻转
- 随机缩放和裁剪
- 色彩空间变换（亮度、对比度、饱和度）
- Mosaic增强（将4张图像拼接成1张）

#### 6.2 超参数调优

可以通过调整以下超参数来优化模型性能：
- **学习率（Learning Rate）**：使用学习率调度策略，初始学习率逐渐衰减
- **优化器（Optimizer）**：可选择SGD、Adam、AdamW等优化器
- **权重衰减（Weight Decay）**：防止过拟合的正则化参数

#### 6.3 模型选择

根据应用场景的需求，可以选择不同大小的YOLO模型：
- **YOLOv8n (nano)**：最小模型，速度最快，适合实时应用
- **YOLOv8s (small)**：小型模型，速度和精度平衡
- **YOLOv8m (medium)**：中型模型，精度较高
- **YOLOv8l (large)**：大型模型，精度最高但速度较慢
- **YOLOv8x (xlarge)**：超大型模型，精度最高

### 7. 实验环境

**硬件环境：**
- GPU: NVIDIA GPU（支持CUDA）
- 显存: 建议8GB以上
- 内存: 建议16GB以上

**软件环境：**
- Python 3.8+
- PyTorch
- Ultralytics YOLO
- CUDA（GPU训练）

**依赖库：**
```
ultralytics
torch
torchvision
opencv-python
numpy
pillow
```

### 8. 总结

本部分详细介绍了基于YOLO的模型训练过程，包括数据集准备、训练脚本编写、参数配置和结果分析。通过使用WIDER FACE数据集和自定义的行为检测数据集，成功训练出了适用于智慧课堂场景的人脸检测和行为检测模型。这些模型为后续的Web应用开发提供了可靠的技术基础。

---

**下一部分：** 第二部分将详细介绍基于Flask的Web应用开发，包括人脸识别、实时视频流处理和用户界面设计等内容。

# 基于YOLO的智慧课堂行为检测系统

## 第二部分：Demo应用开发

### 1. 系统架构

#### 1.1 整体架构

智慧课堂Demo应用采用前后端分离的架构设计，后端使用Flask框架提供RESTful API服务，前端使用HTML、CSS和JavaScript实现用户交互界面。系统主要包含以下三个核心功能模块：

1. **人脸识别打卡模块**：实现学生人脸注册和打卡功能
2. **实时人脸检测模块**：支持视频流和RTSP流的人脸检测与识别
3. **学生行为检测模块**：实时检测学生的课堂行为（低头、转头等）

#### 1.2 技术栈

**后端技术：**
- Flask 2.0+：轻量级Web框架
- Ultralytics YOLO：目标检测模型推理
- FaceNet/InsightFace：人脸特征提取与识别
- OpenCV：图像和视频处理
- PyTorch：深度学习框架

**前端技术：**
- HTML5：页面结构
- CSS3：样式设计
- JavaScript (ES6+)：交互逻辑
- Fetch API：前后端通信

**依赖库（requirements.txt）：**
```
ultralytics>=8.0.0
opencv-python>=4.5.0
numpy>=1.21.0
torch>=2.2.0
torchvision>=0.10.0
flask>=2.0.0
flask-cors>=3.0.0
facenet-pytorch>=2.5.0
insightface>=0.7.3
onnxruntime>=1.15.0
onnxruntime-gpu>=1.15.0
Pillow>=9.0.0
```

### 2. 后端开发

#### 2.1 Flask应用初始化

**核心代码结构：**
```python
from flask import Flask, render_template, request, jsonify, Response
from flask_cors import CORS
from ultralytics import YOLO
import cv2
import numpy as np

app = Flask(__name__)
CORS(app)

# 配置
UPLOAD_FOLDER = 'uploads'
FACES_FOLDER = 'faces_database'
MODEL_FACE_PATH = 'weights/face_detect/best.pt'
MODEL_BEHAVIOR_PATH = 'weights/scb/best.pt'

# 全局变量
face_model = None
behavior_model = None
face_encodings_db = {}
```

#### 2.2 模型加载

系统启动时自动加载训练好的模型：

```python
def init_models():
    global face_model, behavior_model
    
    # 加载人脸检测模型
    if os.path.exists(MODEL_FACE_PATH):
        face_model = YOLO(MODEL_FACE_PATH)
    else:
        face_model = YOLO('yolo11n.pt')  # 使用默认模型
    
    # 加载行为检测模型
    if os.path.exists(MODEL_BEHAVIOR_PATH):
        behavior_model = YOLO(MODEL_BEHAVIOR_PATH)
    
    # 加载已注册的人脸数据库
    load_face_database()
```

#### 2.3 人脸识别模块

##### 2.3.1 人脸检测

使用YOLO模型进行人脸检测：

```python
def detect_faces_yolo(image):
    """使用YOLO检测人脸"""
    if face_model is None:
        return []
    
    results = face_model.predict(image, conf=0.25, verbose=False)
    faces = []
    
    if len(results) > 0 and results[0].boxes is not None:
        boxes = results[0].boxes
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            confidence = float(box.conf[0].cpu().numpy())
            faces.append({
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': confidence
            })
    
    return faces
```

##### 2.3.2 人脸特征提取

系统支持两种人脸识别方案：

**方案一：FaceNet（facenet-pytorch）**
- 使用InceptionResnetV1网络提取512维特征向量
- 支持批量特征提取，提高处理效率

**方案二：InsightFace（ArcFace）**
- 使用buffalo_l模型，提供更高的识别精度
- 支持GPU加速，检测速度更快

**人脸对齐与特征提取：**
```python
def align_face(img, box, target_size=(160, 160)):
    """裁剪并标准化人脸图像"""
    h, w = img.shape[:2]
    x1, y1, x2, y2 = box
    # 边界检查
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)
    face_img = img[y1:y2, x1:x2]
    
    # 转换为RGB并缩放
    face_img_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
    face_img_resized = cv2.resize(face_img_rgb, target_size)
    
    # 转换为张量并标准化
    face_tensor = torch.tensor(face_img_resized).permute(2, 0, 1).float()
    face_tensor = fixed_image_standardization(face_tensor)
    
    return face_tensor

def extract_features_batch(face_tensors):
    """批量提取人脸特征向量"""
    if len(face_tensors) == 0:
        return []
    
    # 堆叠成批次
    batch_tensor = torch.stack(valid_tensors)
    
    # 批量提取特征
    with torch.no_grad():
        device = next(resnet.parameters()).device
        face_encodings = resnet(batch_tensor.to(device))
        # L2归一化
        face_encodings = torch.nn.functional.normalize(face_encodings, p=2, dim=1)
        face_encodings = face_encodings.cpu().numpy()
    
    return face_encodings
```

##### 2.3.3 人脸识别

使用余弦相似度进行人脸匹配：

```python
def recognize_face_from_encoding(face_encoding):
    """识别人脸（与数据库比较）"""
    if len(face_encodings_db) == 0:
        return None, 0.0
    
    best_match = None
    best_similarity = -1.0
    
    for name, known_encoding in face_encodings_db.items():
        # 计算余弦相似度
        similarity = np.dot(face_encoding, known_encoding) / (
            np.linalg.norm(face_encoding) * np.linalg.norm(known_encoding)
        )
        if similarity > best_similarity:
            best_similarity = similarity
            best_match = name
    
    # 相似度阈值：0.6
    if best_similarity > 0.6:
        return best_match, best_similarity
    else:
        return None, best_similarity
```

##### 2.3.4 人脸注册API

```python
@app.route('/api/face/register', methods=['POST'])
def register_face():
    """注册人脸"""
    try:
        file = request.files['image']
        name = request.form['name']
        
        # 读取图片
        file_bytes = file.read()
        nparr = np.frombuffer(file_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        # 检测人脸
        faces = detect_faces_yolo(image)
        if len(faces) == 0:
            return jsonify({'success': False, 'message': '未检测到人脸'}), 400
        
        # 提取特征
        face_box = faces[0]['bbox']
        face_tensor = align_face(image, face_box)
        face_encoding = extract_features(face_tensor)
        
        # 保存到数据库
        face_encodings_db[name] = face_encoding
        save_face_database()
        
        return jsonify({
            'success': True,
            'message': f'成功注册 {name}'
        })
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500
```

##### 2.3.5 人脸打卡API

```python
@app.route('/api/face/checkin', methods=['POST'])
def checkin_face():
    """人脸打卡"""
    try:
        file = request.files['image']
        
        # 读取图片并检测人脸
        file_bytes = file.read()
        nparr = np.frombuffer(file_bytes, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        faces = detect_faces_yolo(image)
        if len(faces) == 0:
            return jsonify({'success': False, 'message': '未检测到人脸'}), 400
        
        # 提取特征并识别
        face_box = faces[0]['bbox']
        face_tensor = align_face(image, face_box)
        face_encoding = extract_features(face_tensor)
        name, similarity = recognize_face_from_encoding(face_encoding)
        
        if name:
            return jsonify({
                'success': True,
                'message': f'打卡成功: {name}',
                'name': name,
                'similarity': similarity,
                'time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        else:
            return jsonify({
                'success': False,
                'message': '未识别到已注册的人脸'
            })
    except Exception as e:
        return jsonify({'success': False, 'message': str(e)}), 500
```

### 3. 实时视频流处理

#### 3.1 视频流架构

系统支持两种视频源：
1. **上传视频文件**：支持mp4、avi、mov、mkv等格式
2. **RTSP流**：支持网络摄像头和IP摄像头的实时流

#### 3.2 帧处理流程

**人脸检测视频流处理：**
```python
def process_frame_face_detection(frame):
    """处理帧进行人脸检测和识别"""
    # 步骤1: YOLO检测人脸
    faces = detect_faces_yolo(frame)
    
    # 步骤2: 对齐所有人脸
    face_tensors = [align_face(frame, face['bbox']) for face in faces]
    
    # 步骤3: 批量提取特征
    face_encodings = extract_features_batch(face_tensors)
    
    # 步骤4: 识别并绘制结果
    annotated_frame = frame.copy()
    for face, encoding in zip(faces, face_encodings):
        name, similarity = recognize_face_from_encoding(encoding)
        # 绘制边界框和标签
        x1, y1, x2, y2 = face['bbox']
        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        if name:
            label = f"{name} ({similarity:.2f})"
            draw_chinese_text(annotated_frame, label, (x1, y1-30))
    
    return annotated_frame
```

**行为检测视频流处理：**
```python
def process_frame_behavior_detection(frame):
    """处理帧进行行为检测"""
    if behavior_model is None:
        return frame
    
    # 使用YOLO检测行为
    results = behavior_model.predict(frame, conf=0.25, verbose=False)
    annotated_frame = frame.copy()
    
    if len(results) > 0 and results[0].boxes is not None:
        boxes = results[0].boxes
        class_names = ['BowHead', 'TurnHead']  # 根据模型调整
        
        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
            cls = int(box.cls[0].cpu().numpy())
            confidence = float(box.conf[0].cpu().numpy())
            
            class_name = class_names[cls] if cls < len(class_names) else f'行为{cls}'
            label = f"{class_name}: {confidence:.2f}"
            
            # 绘制边界框和标签
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
            draw_chinese_text(annotated_frame, label, (x1, y1-30))
    
    return annotated_frame
```

#### 3.3 视频流生成器

使用Flask的Response和生成器实现视频流传输：

```python
def generate_frames_face(source_type, source_path, stream_id):
    """生成人脸检测视频流"""
    if source_type == 'video':
        cap = cv2.VideoCapture(source_path)
    elif source_type == 'rtsp':
        cap = cv2.VideoCapture(source_path)
    else:
        return
    
    video_streams[stream_id] = cap
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # 处理帧
        processed_frame = process_frame_face_detection(frame)
        
        # 编码为JPEG
        ret, buffer = cv2.imencode('.jpg', processed_frame)
        if not ret:
            continue
        
        frame_bytes = buffer.tobytes()
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

@app.route('/api/stream/face/video')
def face_video_stream():
    """人脸检测视频流端点"""
    stream_id = request.args.get('stream_id')
    source_type = request.args.get('source_type')
    source_path = request.args.get('source_path')
    
    return Response(
        generate_frames_face(source_type, source_path, stream_id),
        mimetype='multipart/x-mixed-replace; boundary=frame'
    )
```

### 4. 前端开发

#### 4.1 页面结构

前端采用单页应用（SPA）设计，使用标签页切换不同功能模块：

```html
<div class="container">
    <header>
        <h1>智慧课堂系统</h1>
        <nav>
            <button class="nav-btn active" data-tab="checkin">人脸打卡</button>
            <button class="nav-btn" data-tab="face-detection">人脸检测</button>
            <button class="nav-btn" data-tab="behavior-detection">行为检测</button>
        </nav>
    </header>
    
    <main>
        <!-- 人脸打卡界面 -->
        <div id="checkin" class="tab-content active">
            <!-- 注册人脸 -->
            <!-- 人脸打卡 -->
        </div>
        
        <!-- 人脸检测界面 -->
        <div id="face-detection" class="tab-content">
            <!-- 视频源选择 -->
            <!-- 视频显示 -->
        </div>
        
        <!-- 行为检测界面 -->
        <div id="behavior-detection" class="tab-content">
            <!-- 视频源选择 -->
            <!-- 视频显示 -->
        </div>
    </main>
</div>
```

#### 4.2 人脸注册功能

**前端JavaScript代码：**
```javascript
async function registerFace() {
    const name = document.getElementById('register-name').value;
    const fileInput = document.getElementById('register-image');
    
    if (!name || !fileInput.files[0]) {
        alert('请填写姓名并选择图片');
        return;
    }
    
    const formData = new FormData();
    formData.append('name', name);
    formData.append('image', fileInput.files[0]);
    
    try {
        const response = await fetch('/api/face/register', {
            method: 'POST',
            body: formData
        });
        
        const result = await response.json();
        if (result.success) {
            alert(`注册成功: ${result.message}`);
            document.getElementById('register-name').value = '';
            document.getElementById('register-image').value = '';
        } else {
            alert(`注册失败: ${result.message}`);
        }
    } catch (error) {
        alert('注册失败: ' + error.message);
    }
}
```

#### 4.3 人脸打卡功能

```javascript
async function checkinFace() {
    const fileInput = document.getElementById('checkin-image');
    
    if (!fileInput.files[0]) {
        alert('请选择或拍摄照片');
        return;
    }
    
    const formData = new FormData();
    formData.append('image', fileInput.files[0]);
    
    try {
        const response = await fetch('/api/face/checkin', {
            method: 'POST',
            body: formData
        });
        
        const result = await response.json();
        if (result.success) {
            document.getElementById('checkin-result').innerHTML = `
                <p>打卡成功！</p>
                <p>姓名: ${result.name}</p>
                <p>相似度: ${(result.similarity * 100).toFixed(2)}%</p>
                <p>时间: ${result.time}</p>
            `;
        } else {
            document.getElementById('checkin-result').innerHTML = 
                `<p>打卡失败: ${result.message}</p>`;
        }
    } catch (error) {
        alert('打卡失败: ' + error.message);
    }
}
```

#### 4.4 视频流显示

**启动视频流：**
```javascript
async function startFaceStream() {
    const sourceType = document.getElementById('face-source-type').value;
    const sourcePath = sourceType === 'video' 
        ? document.getElementById('face-video-file').files[0]?.name
        : document.getElementById('face-rtsp-url').value;
    
    if (!sourcePath) {
        alert('请选择视频源');
        return;
    }
    
    // 上传视频文件（如果是文件类型）
    if (sourceType === 'video') {
        const formData = new FormData();
        formData.append('video', document.getElementById('face-video-file').files[0]);
        await fetch('/api/upload/video', {
            method: 'POST',
            body: formData
        });
    }
    
    // 启动视频流
    const streamId = Date.now().toString();
    const videoElement = document.getElementById('face-video');
    videoElement.src = `/api/stream/face/video?stream_id=${streamId}&source_type=${sourceType}&source_path=${sourcePath}`;
}
```

### 5. 性能优化

#### 5.1 批量处理优化

为了提高处理效率，系统采用批量处理策略：

- **批量特征提取**：将多个人脸图像堆叠成批次，一次性通过神经网络，减少GPU调用次数
- **异步处理**：使用线程池处理多个视频流，避免阻塞主线程

```python
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=4)

def process_frame_async(frame):
    future = executor.submit(process_frame_face_detection, frame)
    return future.result()
```

#### 5.2 模型优化

- **模型量化**：使用INT8量化减少模型大小和推理时间
- **TensorRT加速**：在NVIDIA GPU上使用TensorRT进行推理加速
- **多模型策略**：针对不同场景使用不同精度的模型（检测用高精度，注册用快速模型）

#### 5.3 中文显示优化

系统实现了中文字体自动检测和加载功能：

```python
def get_chinese_font(font_size=30):
    """获取中文字体"""
    font_paths = [
        "/usr/share/fonts/truetype/wqy/wqy-microhei.ttc",  # Linux
        "C:/Windows/Fonts/simhei.ttf",  # Windows
        "/System/Library/Fonts/PingFang.ttc",  # macOS
    ]
    
    for font_path in font_paths:
        if os.path.exists(font_path):
            return ImageFont.truetype(font_path, font_size)
    
    return ImageFont.load_default()
```

### 6. 系统部署

#### 6.1 环境配置

**安装依赖：**
```bash
pip install -r requirements.txt
```

**创建必要目录：**
```bash
mkdir -p uploads faces_database weights/face_detect weights/scb
```

#### 6.2 启动服务

**开发模式：**
```bash
cd demo
python app.py
```

**生产模式（使用Gunicorn）：**
```bash
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

#### 6.3 配置文件

系统支持通过环境变量配置：
- `MODEL_FACE_PATH`：人脸检测模型路径
- `MODEL_BEHAVIOR_PATH`：行为检测模型路径
- `UPLOAD_FOLDER`：上传文件存储目录
- `FACES_FOLDER`：人脸数据库目录

### 7. API接口文档

#### 7.1 人脸识别接口

**注册人脸：**
- **URL**: `/api/face/register`
- **方法**: `POST`
- **参数**: 
  - `name` (form-data): 姓名
  - `image` (file): 人脸图片
- **返回**: 
```json
{
    "success": true,
    "message": "成功注册 张三"
}
```

**人脸打卡：**
- **URL**: `/api/face/checkin`
- **方法**: `POST`
- **参数**: 
  - `image` (file): 人脸图片
- **返回**: 
```json
{
    "success": true,
    "message": "打卡成功: 张三",
    "name": "张三",
    "similarity": 0.95,
    "time": "2024-12-23 12:00:00"
}
```

**获取已注册人脸列表：**
- **URL**: `/api/face/list`
- **方法**: `GET`
- **返回**: 
```json
{
    "success": true,
    "faces": ["张三", "李四", "王五"]
}
```

#### 7.2 视频流接口

**启动人脸检测流：**
- **URL**: `/api/stream/face/start`
- **方法**: `POST`
- **参数**: 
  - `source_type`: "video" 或 "rtsp"
  - `source_path`: 视频文件路径或RTSP地址

**获取人脸检测视频流：**
- **URL**: `/api/stream/face/video`
- **方法**: `GET`
- **返回**: MJPEG视频流

**启动行为检测流：**
- **URL**: `/api/stream/behavior/start`
- **方法**: `POST`

**获取行为检测视频流：**
- **URL**: `/api/stream/behavior/video`
- **方法**: `GET`
- **返回**: MJPEG视频流

### 8. 系统测试

#### 8.1 功能测试

- **人脸注册测试**：上传不同角度、光照条件的人脸图片，验证注册成功率
- **人脸识别测试**：使用注册过的人脸进行打卡，验证识别准确率
- **视频流测试**：测试上传视频和RTSP流的实时检测功能
- **行为检测测试**：验证低头、转头等行为的检测准确率

#### 8.2 性能测试

- **响应时间**：测量API接口的响应时间
- **帧率**：测试视频流处理的帧率（FPS）
- **并发性能**：测试多用户同时访问时的系统性能
- **资源占用**：监控CPU、GPU、内存使用情况

### 9. 总结

本部分详细介绍了智慧课堂Demo应用的开发过程，包括：

1. **系统架构设计**：采用Flask后端 + 前端分离架构
2. **核心功能实现**：人脸识别、实时视频流处理、行为检测
3. **性能优化**：批量处理、模型优化、中文显示优化
4. **部署方案**：环境配置、服务启动、API文档

系统成功实现了智慧课堂的核心功能，为教师提供了便捷的课堂管理工具。通过YOLO模型的高效检测和FaceNet/InsightFace的精确识别，系统在保证准确性的同时，也具备了良好的实时性能。

---

**完整系统说明：**
- 第一部分：模型训练（已完成）
- 第二部分：Demo应用开发（本部分）

系统已完整实现从模型训练到应用部署的全流程，为智慧课堂提供了完整的技术解决方案。

```

```

